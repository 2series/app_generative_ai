{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whjsJasuhstV"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/app_generative_ai/blob/main/t81_559_class_02_4_software_eng.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euOZxlIMhstX"
   },
   "source": [
    "# T81-559: Applications of Generative Artificial Intelligence\n",
    "**Module 2: Code Generation**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4Yov72PhstY"
   },
   "source": [
    "# Module 2 Material\n",
    "\n",
    "* Part 2.1: Prompting for Code Generation [[Video]]() [[Notebook]](t81_559_class_02_1_dev.ipynb)\n",
    "* Part 2.2: Handling Revision Prompts [[Video]]() [[Notebook]](t81_559_class_02_2_multi_prompt.ipynb)\n",
    "* Part 2.3: Using a LLM to Help Debug [[Video]]() [[Notebook]](t81_559_class_02_3_llm_debug.ipynb)\n",
    "* **Part 2.4: Tracking Prompts in Software Development** [[Video]]() [[Notebook]](t81_559_class_02_4_software_eng.ipynb)\n",
    "* Part 2.5: Limits of LLM Code Generation [[Video]]() [[Notebook]](t81_559_class_02_5_code_gen_limits.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcAUP0c3hstY"
   },
   "source": [
    "# Google CoLab Instructions\n",
    "\n",
    "The following code ensures that Google CoLab is running and maps Google Drive if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xsI496h5hstZ",
    "outputId": "4c950075-215a-464b-f04b-7a831d498767"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: using Google CoLab\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.1.16-py3-none-any.whl (817 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.7/817.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langchain_openai\n",
      "  Downloading langchain_openai-0.1.4-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.29)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
      "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting langchain-community<0.1,>=0.0.32 (from langchain)\n",
      "  Downloading langchain_community-0.0.34-py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langchain-core<0.2.0,>=0.1.42 (from langchain)\n",
      "  Downloading langchain_core-0.1.46-py3-none-any.whl (299 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.3/299.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.1.51-py3-none-any.whl (115 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.0/116.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.0)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
      "Collecting openai<2.0.0,>=1.10.0 (from langchain_openai)\n",
      "  Downloading openai-1.23.6-py3-none-any.whl (311 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.6/311.6 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tiktoken<1,>=0.5.2 (from langchain_openai)\n",
      "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
      "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.42->langchain)\n",
      "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading orjson-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m939.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (1.7.0)\n",
      "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.10.0->langchain_openai)\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (4.11.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.5.2->langchain_openai) (2023.12.25)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.10.0->langchain_openai) (1.2.1)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai)\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: packaging, orjson, mypy-extensions, jsonpointer, h11, typing-inspect, tiktoken, marshmallow, jsonpatch, httpcore, langsmith, httpx, dataclasses-json, openai, langchain-core, langchain-text-splitters, langchain_openai, langchain-community, langchain\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.0\n",
      "    Uninstalling packaging-24.0:\n",
      "      Successfully uninstalled packaging-24.0\n",
      "Successfully installed dataclasses-json-0.6.4 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.16 langchain-community-0.0.34 langchain-core-0.1.46 langchain-text-splitters-0.0.1 langchain_openai-0.1.4 langsmith-0.1.51 marshmallow-3.21.1 mypy-extensions-1.0.0 openai-1.23.6 orjson-3.10.1 packaging-23.2 tiktoken-0.6.0 typing-inspect-0.9.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    from google.colab import drive, userdata\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False\n",
    "\n",
    "# OpenAI Secrets\n",
    "if COLAB:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "# Install needed libraries in CoLab\n",
    "if COLAB:\n",
    "    !pip install langchain langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pC9A-LaYhsta"
   },
   "source": [
    "# 2.4: Tracking Prompts in Software Development\n",
    "\n",
    "Prompting will undoubtedly become an essential part of modern software engineering. Programmers will likely construct individual prompts to generate methods. In this part, we will see how to write a prompt that consistently produces a non-trivial image cropping function. We will also store the prompt with the function so we do not lose the prompt in the future. Additionally, we will use automated unit tests to ensure that the generated method initially does what we expect, and continues to do so in the future, even if the technique is regenerated.\n",
    "\n",
    "## Conversational Code Generation\n",
    "\n",
    "We will continue to use the conversational code generation function provided in Module 2.2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TMF-rtxgRAea"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts.chat import PromptTemplate\n",
    "from IPython.display import display_markdown\n",
    "\n",
    "MODEL = 'gpt-4o-mini'\n",
    "TEMPLATE = \"\"\"The following is a friendly conversation between a human and an\n",
    "AI to generate Python code. If you have notes about the code, place them before\n",
    "the code. Any nots about execution should follow the code. If you do mix any\n",
    "notes with the code, make them comments. Add proper comments to the code.\n",
    "Sort imports and follow PEP-8 formatting.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "Human: {input}\n",
    "Code Assistant:\"\"\"\n",
    "PROMPT_TEMPLATE = PromptTemplate(input_variables=[\"history\", \"input\"], template=TEMPLATE)\n",
    "\n",
    "def start_conversation():\n",
    "    # Initialize the OpenAI LLM with your API key\n",
    "    llm = ChatOpenAI(\n",
    "        model=MODEL,\n",
    "        temperature=0.0,\n",
    "        n=1\n",
    "    )\n",
    "\n",
    "    # Initialize memory and conversation\n",
    "    memory = ConversationBufferWindowMemory()\n",
    "    conversation = ConversationChain(\n",
    "        prompt=PROMPT_TEMPLATE,\n",
    "        llm=llm,\n",
    "        memory=memory,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    return conversation\n",
    "\n",
    "def generate_code(conversation, prompt):\n",
    "    print(\"Model response:\")\n",
    "    output = conversation.invoke(prompt)\n",
    "    display_markdown(output['response'], raw=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ze2LsFCScSQW"
   },
   "source": [
    "## Prompts for Consistent Code Generation\n",
    "\n",
    "The code below shows the prompt I created to generate the clipping function. It is essential to specify very clearly what you want from the LLM. In this case, I made sure to specify:\n",
    "\n",
    "* The name of the function\n",
    "* The name of all arguments to that function\n",
    "* What to return\n",
    "* The exact algorithm, including how to handle negative values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 799
    },
    "id": "6bkupDjIbni5",
    "outputId": "ecdeeb80-5e60-420d-9985-4304e6eae56a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "import numpy as np\n",
       "\n",
       "def safe_clip(cv2_image, x, y, width, height, background):\n",
       "    \"\"\"\n",
       "    Clips a region from an OpenCV image, adjusting for boundaries and filling missing areas.\n",
       "\n",
       "    Parameters:\n",
       "        cv2_image (numpy.ndarray): The source image from which to clip the region.\n",
       "        x (int): The x-coordinate of the top-left corner of the clipping region.\n",
       "        y (int): The y-coordinate of the top-left corner of the clipping region.\n",
       "        width (int): The width of the clipping region.\n",
       "        height (int): The height of the clipping region.\n",
       "        background (tuple): A tuple representing the BGR color (e.g., (255, 255, 255) for white) used to fill missing areas.\n",
       "\n",
       "    Returns:\n",
       "        tuple: A tuple containing:\n",
       "            - new_image (numpy.ndarray): The new image with the clipped region and background.\n",
       "            - x_offset (int): The x-coordinate offset of the new image relative to the original image.\n",
       "            - y_offset (int): The y-coordinate offset of the new image relative to the original image.\n",
       "\n",
       "    Example:\n",
       "        >>> img = np.zeros((100, 100, 3), dtype=np.uint8)\n",
       "        >>> clipped_img, x_off, y_off = safe_clip(img, -10, -10, 120, 120, (255, 0, 0))\n",
       "        >>> print(clipped_img.shape, x_off, y_off)\n",
       "        ((120, 120, 3), 10, 10)\n",
       "    \"\"\"\n",
       "    # Calculate effective x, y, width, and height considering image boundaries\n",
       "    x_eff = max(x, 0)\n",
       "    y_eff = max(y, 0)\n",
       "    width_eff = min(width, cv2_image.shape[1] - x_eff)\n",
       "    height_eff = min(height, cv2_image.shape[0] - y_eff)\n",
       "\n",
       "    # Create a new image filled with the background color\n",
       "    new_image = np.full((height, width, 3), background, dtype=cv2_image.dtype)\n",
       "\n",
       "    # Calculate offsets if the requested region is out of the original image bounds\n",
       "    x_offset = -min(x, 0)\n",
       "    y_offset = -min(y, 0)\n",
       "\n",
       "    # Place the clipped part of the original image into the new image\n",
       "    new_image[y_offset:y_offset + height_eff, x_offset:x_offset + width_eff] = \\\n",
       "        cv2_image[y_eff:y_eff + height_eff, x_eff:x_eff + width_eff]\n",
       "\n",
       "    return new_image, x_offset, y_offset\n",
       "```"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conversation = start_conversation()\n",
    "generate_code(conversation,\"\"\"\n",
    "Write a Python function named safe_clip using the PEP-8 style guide. The function\n",
    "should accept five parameters: cv2_image, x, y, width, height, and background.\n",
    "The imports should be sorted alphabetically.\n",
    "\n",
    "The purpose of this function is to clip a region from an OpenCV image while a\n",
    "djusting for boundaries and filling any areas missing from the original image\n",
    "boundaries with a specified color. Ensure that the code includes comments and a\n",
    "docstring explaining the functionality in detail.\n",
    "\n",
    "The function should:\n",
    "\n",
    "Calculate the dimensions of the clipping region and adjust them if they extend\n",
    "beyond the image boundaries. Negative x and y indicate that the source image\n",
    "must be expanded to accomodate the these coordinate that were outside the image.\n",
    "Create a new image of the specified size, filled with the background color.\n",
    "Place the clipped region into the newly created image at the appropriate location.\n",
    "Return a tuple containing the new image, along with x and y offsets indicating\n",
    "how much the origin of the clipped image has shifted relative to the original image.\n",
    "The return type should be specified in the function's docstring, which should\n",
    "also detail the types and purposes of each parameter. Additionally, mention that\n",
    "the function will adjust coordinates and size to fit within the image dimensions and\n",
    "fill missing areas with the specified background color.\n",
    "\n",
    "Include a detailed example in the docstring illustrating how the function is\n",
    "called and what it returns.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OCmjBxIdcesJ"
   },
   "source": [
    "## Trying Out the Generated Code\n",
    "\n",
    "The code below shows the prompt I created to generate the clipping function. It is essential to specify very clearly what you want from the LLM. In this case, I made sure to specify:\n",
    "\n",
    "* The name of the function\n",
    "* The name of all arguments to that function\n",
    "* What to return\n",
    "* The exact algorithm, including how to handle negative values\n",
    "\n",
    "The following code shows my function; I also embed the prompt inside the source as a comment. This way, I can track changes to both the function and prompt as I check both in a source code repository like GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "q42Lx21EcjTf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "## safe_clip was generated by the following prompt:\n",
    "# Write a Python function named safe_clip using the PEP-8 style guide. The function\n",
    "# should accept five parameters: cv2_image, x, y, width, height, and background.\n",
    "# The imports should be sorted alphabetically.\n",
    "#\n",
    "# The purpose of this function is to clip a region from an OpenCV image while a\n",
    "# djusting for boundaries and filling any areas missing from the original image\n",
    "# boundaries with a specified color. Ensure that the code includes comments and a\n",
    "# docstring explaining the functionality in detail.\n",
    "#\n",
    "# The function should:\n",
    "#\n",
    "# Calculate the dimensions of the clipping region and adjust them if they extend\n",
    "# beyond the image boundaries. Negative x and y indicate that the source image\n",
    "# must be expanded to accomodate the these coordinate that were outside the image.\n",
    "# Create a new image of the specified size, filled with the background color.\n",
    "# Place the clipped region into the newly created image at the appropriate location.\n",
    "# Return a tuple containing the new image, along with x and y offsets indicating\n",
    "# how much the origin of the clipped image has shifted relative to the original image.\n",
    "# The return type should be specified in the function's docstring, which should\n",
    "# also detail the types and purposes of each parameter. Additionally, mention that\n",
    "# the function will adjust coordinates and size to fit within the image dimensions and\n",
    "# fill missing areas with the specified background color.\n",
    "#\n",
    "# Include a detailed example in the docstring illustrating how the function is\n",
    "# called and what it returns.\n",
    "\n",
    "def safe_clip(cv2_image, x, y, width, height, background):\n",
    "    \"\"\"\n",
    "    Clips a region from an OpenCV image, adjusting for boundaries and filling missing areas.\n",
    "\n",
    "    Parameters:\n",
    "        cv2_image (numpy.ndarray): The source image from which to clip the region.\n",
    "        x (int): The x-coordinate of the top-left corner of the clipping region.\n",
    "        y (int): The y-coordinate of the top-left corner of the clipping region.\n",
    "        width (int): The width of the clipping region.\n",
    "        height (int): The height of the clipping region.\n",
    "        background (tuple): A tuple representing the BGR color (e.g., (255, 255, 255) for white) used to fill missing areas.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - new_image (numpy.ndarray): The new image with the clipped region and background.\n",
    "            - x_offset (int): The x-coordinate offset of the new image relative to the original image.\n",
    "            - y_offset (int): The y-coordinate offset of the new image relative to the original image.\n",
    "\n",
    "    Example:\n",
    "        >>> img = np.zeros((100, 100, 3), dtype=np.uint8)\n",
    "        >>> clipped_img, x_off, y_off = safe_clip(img, -10, -10, 120, 120, (255, 0, 0))\n",
    "        >>> print(clipped_img.shape, x_off, y_off)\n",
    "        ((120, 120, 3), 10, 10)\n",
    "    \"\"\"\n",
    "    # Calculate effective x, y, width, and height considering image boundaries\n",
    "    x_eff = max(x, 0)\n",
    "    y_eff = max(y, 0)\n",
    "    width_eff = min(width, cv2_image.shape[1] - x_eff)\n",
    "    height_eff = min(height, cv2_image.shape[0] - y_eff)\n",
    "\n",
    "    # Create a new image filled with the background color\n",
    "    new_image = np.full((height, width, 3), background, dtype=cv2_image.dtype)\n",
    "\n",
    "    # Calculate offsets if the requested region is out of the original image bounds\n",
    "    x_offset = -min(x, 0)\n",
    "    y_offset = -min(y, 0)\n",
    "\n",
    "    # Place the clipped part of the original image into the new image\n",
    "    new_image[y_offset:y_offset + height_eff, x_offset:x_offset + width_eff] = \\\n",
    "        cv2_image[y_eff:y_eff + height_eff, x_eff:x_eff + width_eff]\n",
    "\n",
    "    return new_image, x_offset, y_offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0U3wWY3jv1it"
   },
   "source": [
    "Now, I will create an image of a grayscale and show how the function performs on three different types of crops. One of the critical features of this cropping function is that it might expand the original image if the upper-left corner or lower-right corner is off of the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "id": "DcmhBIZQclni",
    "outputId": "78fd1db1-c906-4780-8255-7071709a818c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABOwAAAE1CAYAAABUTH/FAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/TGe4hAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAvg0lEQVR4nO3de7xVc94H8O8+3TtdVEqhQii3kIR4uiCTjEGixjQSuRTPMJNpeMbQYFBCngdjGPSYl3EdGhLDqIZRUeM2uYzHzFRjKIlcGrqcfs8ftbezz6VONy3m/X699uucs9Z3rfVba+/9O2t/9rrkUkopAAAAAIBMKNnSDQAAAAAAviCwAwAAAIAMEdgBAAAAQIYI7AAAAAAgQwR2AAAAAJAhAjsAAAAAyBCBHQAAAABkiMAOAAAAADJEYAcAAAAAGSKw20xmzpwZJ5xwQrRp0ybq1q0brVu3jgEDBsSMGTPWaz6jR4+OXC63QW2YNm1a5HK5mDZt2gZNX1O9evWKXr161ahuzz333KxtAYq98sorMXTo0Nhxxx2jfv360ahRo+jSpUuMHTs2Pvjgg0JdVe/jXC4Xo0eP/nIbvAF22GGHOOWUU6odX1ZWFltttVUceeSRlcZdd911kcvl4tvf/nalcZdddlnkcrl45ZVXqlzOO++8E6NHj46XXnqp0rSnnHJKNGrUqEbt39TbOf9/I/8oKSmJNm3aRL9+/eLZZ5/dZMvZXObOnRu5XC4mTJiwpZsCG0zfW9n7778f9erVi1wuF7Nnz66y5oorroiJEyfWeJ7l+7qKj5q07ZRTTokddtihxsuDrxL9ULHFixfHhRdeGLvvvns0bNgwmjRpEgceeGDceOONsWLFig1uw+TJkzd4W63PPs/G5AJVmTBhQqV+s2XLltGrV6+YNGnSJlvO5vRVeZ1ujNpbugFfR//zP/8T5513XnTr1i3Gjh0b7du3j/nz58eNN94YhxxySFx//fVxzjnn1Ghew4YNi759+25QO7p06RIzZsyI3XfffYOmB77abr311hgxYkR07NgxfvjDH8buu+8eK1asiNmzZ8fNN98cM2bMiIceeqja6WfMmBHbb7/9l9jizaNWrVrxH//xHzFt2rRYuXJl1K79xb++adOmRWlpaUydOrXSdNOmTYsWLVrEXnvtFRERDz30UDRp0qQw/p133omf/vSnscMOO8Q+++yzwe3bXNv58ccfj6ZNm8aqVati/vz5MXbs2OjVq1c899xz0aVLl02+PGA1fW/VfvWrX8Xy5csjIuK2226Lrl27Vqq54oorYsCAAXHsscfWeL4DBgyIkSNHVhresmXLDW4rfNXph4q98cYbccQRR8Snn34aI0eOjO7du8dnn30WkyZNinPPPTfuv//+mDx5cjRs2HC95z158uS48cYbNyg4atOmTcyYMSM6dOiw3tNuKnfccUd06tQpUkqxYMGCuOGGG+Loo4+Ohx9+OI4++ugt1i5WE9htYs8++2ycd9550a9fv3jooYeKPhgOGjQojjvuuDj33HNj3333jYMPPrja+fzrX/+Khg0bxvbbb7/BnWX+WwPg38+MGTNi+PDh0adPn5g4cWLUq1evMK5Pnz4xcuTIePzxx9c6j69T/9G7d++YNGlSzJ49u7Beq1atimeeeSaGDx8e48aNi9dffz122223iIhYvnx5zJgxI/r161f4NnPffffdLG3bXNt5v/32i6233joiIrp37x7dunWLDh06xAMPPCCwg81E31u922+/PVq1ahXt27ePu+++O6699tpo0KDBRs93m222+dpuM9gQ+qFiZWVlcfzxx8fHH38czz//fOy6666Fcf369YuePXvGoEGD4gc/+EHcfPPNX2rb6tWrt8W39Z577ln0BUrfvn2jWbNmcffddwvsMsApsZvYlVdeGblcLn7+858XhXUREbVr146bbropcrlcXHXVVYXh+cNbX3jhhRgwYEA0a9askLJXdejrsmXLYuTIkdG6deto2LBh9OjRI/70pz9VOiS4qlNi86dpvfXWW9GvX79o1KhRtG3bNkaOHBnLli0rWs5Pf/rTOOCAA6J58+bRpEmT6NKlS9x2222RUtpEW2v1YaznnHNO3HHHHdGxY8do0KBBdO3aNWbOnBkppbj66qtjxx13jEaNGsWhhx4ab731VtH0Tz75ZBxzzDGx/fbbR/369WPnnXeOM888M95///1Ky/rtb38bnTt3jnr16sVOO+0U119/fZXbN6UUN910U+yzzz7RoEGDaNasWQwYMCD+9re/bbL1hs3tiiuuiFwuF7fcckvRjlpe3bp141vf+tZa51HxMPP8ofNPPvlkDB06NJo3bx6lpaVx9NFHV3p/5E+Bf+aZZ+LAAw+MBg0axHbbbRc/+clPoqysrKh2+fLlcfnll0enTp2iXr160bJlyxg6dGgsWrSoqG7FihUxatSoQt93yCGHxPPPP1+j7dG7d++IiKL+8OWXX44PP/wwzjjjjGjTpk3RUXbPPfdcfPbZZ4XpIopPu5g2bVrsv//+ERExdOjQwqkEFb9drUlfW912njp1agwfPjy23nrraNGiRfTv3z/eeeedGq1vVZo2bRoREXXq1CkaPn/+/Bg8eHC0atUq6tWrF7vttltcc801sWrVqkJNdZdYqOpUjvX5P/POO+/EiSeeGI0bN46mTZvGwIEDY8GCBZXa/re//S0GDRoU2267bdSrVy+22WabOOyww6o8HRm2JH1v1Z577rmYM2dOfPe7343TTz89Pvroo/jNb35Tab2XLl0a//u//1voU2tyyZWamjBhQnTs2LHQz915551V1r399tsxYMCAaNy4cWy11Vbxne98J2bNmlXlaWuzZ8+Ob33rW9G8efOoX79+7LvvvnHfffdtsjbDhtAPFXvooYfitddeiwsuuKAorMsbOHBgHHHEEXHbbbcV9kFqut9zyimnxI033ljYZvnH3LlzIyLi/vvvjwMOOCCaNm0aDRs2jJ122ilOPfXUaueX9+ijj8Y+++wT9erVix133DHGjRtX5bptjs+t9evXj7p161baX/zggw9ixIgRsd1220XdunVjp512ih//+MdF+3ZrO8W34msq/zn81VdfjW9/+9vRtGnT2GabbeLUU0+Njz76qGjajz/+OE4//fRo0aJFNGrUKPr27RtvvvlmpWUsWrQozjjjjGjbtm3h9XTwwQfH73//+w3eHluaI+w2obKyspg6dWp07dq12qPi2rZtG/vtt19MmTIlysrKolatWoVx/fv3j0GDBsVZZ50VS5curXY5Q4cOjXvvvTdGjRoVhx56aLz22mtx3HHHxccff1yjdq5YsSK+9a1vxWmnnRYjR46Mp59+Oi677LJo2rRpXHzxxYW6uXPnxplnnhnt2rWLiNXX5fvP//zP+Oc//1lUt7EmTZoUL774Ylx11VWRy+XiRz/6URx11FExZMiQ+Nvf/hY33HBDfPTRR/GDH/wgjj/++HjppZcKIdtf//rXOOigg2LYsGHRtGnTmDt3blx77bVxyCGHxJ///OdCR/P4449H//79o0ePHnHvvffGypUrY9y4cbFw4cJK7TnzzDNjwoQJ8b3vfS/GjBkTH3zwQVx66aXRvXv3ePnll2ObbbbZZOsOm0NZWVlMmTIl9ttvv2jbtu0mn/9pp50Wffr0iV//+tfxj3/8Iy666KLo1atXvPLKK7HVVlsV6hYsWBCDBg2KCy64IC699NJ49NFH4/LLL48PP/wwbrjhhohYfZTbMcccE88880yMGjUqunfvHvPmzYtLLrkkevXqFbNnzy4cgXH66afHnXfeGeeff3706dMn5syZE/37949PPvlknW3ee++9o1mzZjF16tS44IILIiJi6tSp0aZNm9hll12iR48eMW3atBgxYkRhXEQUBXbldenSJe64444YOnRoXHTRRXHUUUdFRBT1/TXta6szbNiwOOqoowrb+Yc//GEMHjw4pkyZss5pI1a/DlauXFk4Jfaiiy6KevXqxYABAwo1ixYtiu7du8fy5cvjsssuix122CEmTZoU559/fvz1r3+Nm266qUbLqqgm6/7ZZ5/F4YcfHu+8805ceeWVseuuu8ajjz4aAwcOrDS/fv36RVlZWYwdOzbatWsX77//fkyfPj2WLFmyQe2DzUHfW73bbrstIiJOPfXUaNu2bZx33nlx2223xeDBgws1M2bMiEMPPTR69+4dP/nJTyIiii5DUJ2UUqxcubLS8Fq1ahX2FydMmBBDhw6NY445Jq655pr46KOPYvTo0bFs2bIoKfni+IWlS5dG796944MPPogxY8bEzjvvHI8//niV/dLUqVOjb9++ccABB8TNN98cTZs2jXvuuScGDhwY//rXv9br+n6wqeiHKnvyyScjItZ6qv2xxx4bTzzxREybNi0GDRpU4+3xk5/8JJYuXRoPPPBA0bXq86e6Dhw4MAYOHBijR4+O+vXrx7x589a5H/fUU0/FMcccEwcddFDcc889hf2fzfW5Nb+/mFKKhQsXxtVXXx1Lly6Nk046qVDz+eefR+/eveOvf/1r/PSnP43OnTvHM888E1deeWW89NJL8eijj9Z4m1V0/PHHx8CBA+O0006LP//5z3HhhRdGxOqjsiNW9/HHHntsTJ8+PS6++OLYf//949lnn63y2tTf/e5344UXXoif/exnseuuu8aSJUvihRdeiMWLF29w+7a4xCazYMGCFBFp0KBBa60bOHBgioi0cOHClFJKl1xySYqIdPHFF1eqzY/Le/XVV1NEpB/96EdFdXfffXeKiDRkyJDCsKlTp6aISFOnTi0MGzJkSIqIdN999xVN369fv9SxY8dq21xWVpZWrFiRLr300tSiRYu0atWqwriePXumnj17rnWd83V77LFH0bCISK1bt06ffvppYdjEiRNTRKR99tmnaDnjx49PEZFeeeWVKue/atWqtGLFijRv3rwUEem3v/1tYdz++++f2rZtm5YtW1YY9sknn6QWLVoUbd8ZM2akiEjXXHNN0bz/8Y9/pAYNGqRRo0atcz1hS6tpX1ReVe/jiEiXXHJJ4e877rgjRUQ67rjjiuqeffbZFBHp8ssvL5pfxfdhSimdfvrpqaSkJM2bNy+l9EXf9Zvf/KaobtasWSki0k033ZRSSun1119PEZG+//3vF9Xdddddlfq+6hx77LGptLQ0rVixIqWU0tFHH13YRjfddFNq2bJloc/p3bt3atWqVdH07du3L1pOvo133HFHpWWtT19b3XYeMWJEUd3YsWNTRKR33313reuZ/79R8dGkSZP04IMPFtVecMEFKSLSc889VzR8+PDhKZfLpb/85S8ppar/n6SU0t///vdK26Cm6/7zn/+82tdI+Xm+//77KSLS+PHj17resKXpe6u2dOnS1KRJk3TggQcWhg0ZMiTlcrn01ltvFdWWlpbWaJ55VfV1+cevfvWrlNLqfdhtt902denSpWi/cu7cualOnTqpffv2hWE33nhjioj02GOPFS3nzDPPrNTXderUKe27776F/yl53/zmN1ObNm1SWVlZjdcDNhX9UGV9+/ZNEZE+//zzamsee+yxFBFpzJgxKaX12+85++yziz5P5o0bNy5FRFqyZEm1y61qfgcccEDadttt02effVYY9vHHH6fmzZtv0s+t+ee04qNevXqFbZ938803V7lvN2bMmBQR6Yknnqh2ffIqvqby+6tjx44tqhsxYkSqX79+ob/OPzfXX399Ud3PfvazSvNs1KhROu+889a63l81TondAtKaU0ornop5/PHHr3PaP/zhDxERceKJJxYNHzBgQKVTcKuTy+UqnY/euXPnmDdvXtGwKVOmxOGHHx5NmzaNWrVqRZ06deLiiy+OxYsXx3vvvVejZdVE7969o7S0tPB3/hpSRx55ZNE2yg8v38733nsvzjrrrGjbtm3Url076tSpE+3bt4+IiNdffz0iVn9bOnv27Dj22GOjbt26hWkbNWpUaTtMmjQpcrlcDB48OFauXFl4tG7dOvbee+/Nfsdd+Cr4zne+U/R39+7do3379pVu3NC4ceNKp1ycdNJJsWrVqnj66acjYvV7bquttoqjjz666D23zz77ROvWrQvvufy8Ky77xBNPrHHf17t371i6dGnMmjWrcP26/OlWPXv2jEWLFsWrr74ay5Yti5kzZ1Z7dF1N1bSvrU7Fbde5c+eIiBpP//vf/z5mzZoVzz//fEyaNCkOP/zwGDRoUNFFpqdMmRK77757dOvWrWjaU045JVJKNT6ar6KarPvUqVOrfY2U17x58+jQoUNcffXVce2118aLL75YdLou/Lv4qva99913X3z88cdFp4GdeuqpkVKKO+64o0bzWJsTTzwxZs2aVenRr1+/iIj4y1/+Eu+8806cdNJJRfuV7du3j+7duxfN6w9/+EM0bty40g3fKt5J/K233oo33nijsF3Kb8N+/frFu+++G3/5y182et0ga76q/dC6VPf5fGPkL51y4oknxn333Rf//Oc/1zlNfj+1f//+Ub9+/cLwxo0bb7bPrXfeeWeh33zsscdiyJAhcfbZZxeOhIxYvb9YWlpadJZGRBSOJH7qqadqtKyqVLW/+/nnnxfyhuqe/4r7ixER3bp1iwkTJsTll18eM2fO3Ki7/2aFwG4T2nrrraNhw4bx97//fa11c+fOjYYNG0bz5s2Lhrdp02ady8gfzlnx8NbatWtHixYtatTOhg0bFnUAEasvePn5558X/n7++efjiCOOiIjVdxl69tlnY9asWfHjH/84IlafyrSpVNwO+VCtuuH5dq5atSqOOOKIePDBB2PUqFHx1FNPxfPPPx8zZ84sauOHH34YKaUqDwmuOGzhwoWF2jp16hQ9Zs6cWeW18SBratoXbajWrVtXOazi4eZVvefy0+ZrFy5cGEuWLClcK6P8Y8GCBYX3XL6+4rLXp+/LB3BTp06NF198MZYsWRI9e/aMiIjdd989WrZsGdOmTYuZM2dWun7dhqhJX7s2Fdcrfx2amva/e++9d3Tt2jX233//OOqoo+L++++PnXfeOc4+++xCzeLFi6v837PtttsWxm+Imqz74sWL1/oaycvlcvHUU0/FN77xjRg7dmx06dIlWrZsGd/73vfW65Q82Nz0vVW77bbbon79+tG3b99YsmRJLFmyJDp37hw77LBDTJgwodI1rdZXy5Yto2vXrpUe+f3I6tahqmHV9UtV7S9GRJx//vmVtl/+0gr2GdkS9EOV5S/vtLZtkr/m3KY8jbhHjx4xceLEWLlyZZx88smx/fbbx5577hl33313tdN8+OGHsWrVqhr1V5vqc+tuu+1W6Df79u0bv/jFL+KII46IUaNGFS49snjx4mjdunWlQLNVq1ZRu3btjTrldF37u4sXL67yua5qG917770xZMiQ+OUvfxkHHXRQNG/ePE4++eQqr4/8VeEadptQrVq1onfv3vH444/H22+/XeV17N5+++3405/+FEceeWTR9esiapbo51+oCxcujO22264wfOXKlZv03Ox77rkn6tSpE5MmTSr60DVx4sRNtoyNNWfOnHj55ZdjwoQJMWTIkMLwijemaNasWeRyuSrP+6/45t16660jl8vFM888U+VFWqsaBllTq1atOOyww+Kxxx6rti/aGFX901uwYEHsvPPORcPW9p7L92X5GypUd7eyxo0bF9UvWLBgg/u+PffcsxDK5W9c0KlTp8L4Hj16xNSpUwvz29jALmtKSkpijz32iPvvvz/ee++9aNWqVbRo0SLefffdSrX5m1vk7zKb/z9Q8aYRG/OBtEWLFlVeMLqq11f79u0L18B6880347777ovRo0fH8uXLv/Q7ukF19L2Vvfnmm/HHP/4xIr740FzR7373u8LRcJtD+XWoqOKwmvZL+b7xwgsvjP79+1e53I4dO25Qe2Fj6Icq69OnT9xyyy0xceLEwnWMK5o4cWLUrl27cObFptrvOeaYY+KYY44pnL1x5ZVXxkknnRQ77LBDHHTQQZXq859ba9Jfbc7PrZ07d47f/e538eabb0a3bt2iRYsW8dxzz0VKqSizeO+992LlypXr3F/c2EAv/1yXD+2q2kZbb711jB8/PsaPHx/z58+Phx9+OC644IJ477331nln5KxyhN0mduGFF0ZKKUaMGFHpG8OysrIYPnx4pJQKF1NcXz169IiI1elxeQ888ECVF9zdULlcLmrXrl0UKn722Wfxq1/9apMtY2PlO4uKndEvfvGLor9LS0uja9euMXHixFi+fHlh+KeffhqTJk0qqv3mN78ZKaX45z//WeW3tXvttddmWhvYtPJ90emnn170us9bsWJFPPLIIxs077vuuqvo7+nTp8e8efMq3c3vk08+iYcffrho2K9//esoKSkp9GXf/OY3Y/HixVFWVlbley7/gSc/74rLvu+++2rc9+VyuejZs2dMnz49nnzyycLRdXk9e/aMP/zhDzF16tTYdtttq7yTWHnre8TbllZWVhZ//vOfo169eoULuR922GHx2muvxQsvvFBUe+edd0YulyuEljvssENERLzyyitFdRWf3/XRu3fval8ja7PrrrvGRRddFHvttVeldsOWpu8tlg/ab7311pg6dWrRY/LkyVGnTp3ChcUjVverm7pP7dixY7Rp0ybuvvvuwmlvEasvLzB9+vSi2p49e8Ynn3wSjz32WNHwe+65p9I8d9lll3j55Zer3H5du3YthA3wZdMPFTvuuONi9913j6uuuqrKO4vee++98cQTT8SwYcMKR22tz35PTfYH69WrFz179owxY8ZERMSLL75YZV1paWl069YtHnzwwaKzEj755JNKz9nm/Nz60ksvRcTqI5gjVu8vfvrpp5UO3snfbfuwww6LiNVHVtavX7/Sdvvtb3+7wW3J74tWfP7Xtb/Yrl27OOecc6JPnz5f6f1FR9htYgcffHCMHz8+zjvvvDjkkEPinHPOiXbt2sX8+fPjxhtvjOeeey7Gjx9f6ZoZNbXHHnvEt7/97bjmmmuiVq1aceihh8arr74a11xzTTRt2rToTlcb46ijjoprr702TjrppDjjjDNi8eLFMW7cuEwdYdapU6fo0KFDXHDBBZFSiubNm8cjjzxSuBNQeZdeemkcddRR8Y1vfCPOPffcKCsri6uvvjoaNWoUH3zwQaHu4IMPjjPOOCOGDh0as2fPjh49ekRpaWm8++678cc//jH22muvGD58+Je5mrBBDjrooPj5z38eI0aMiP322y+GDx8ee+yxR6xYsSJefPHFuOWWW2LPPfesdD2Mmpg9e3YMGzYsTjjhhPjHP/4RP/7xj2O77bYrnAaU16JFixg+fHjMnz8/dt1115g8eXLceuutMXz48MKRFoMGDYq77ror+vXrF+eee25069Yt6tSpE2+//XZMnTo1jjnmmDjuuONit912i8GDB8f48eOjTp06cfjhh8ecOXNi3LhxNbqLYF7v3r3jgQceiCeeeKLo2hwRqz+oLV68OJ5++ukqr4tRUYcOHaJBgwZx1113xW677RaNGjWKbbfdtnA66Zb2pz/9KZo2bRoRq7/pvv322+ONN96I73//+4VvQL///e/HnXfeGUcddVRceuml0b59+3j00UfjpptuiuHDhxdCy9atW8fhhx8eV155ZTRr1izat28fTz31VDz44IMb3L6TTz45rrvuujj55JPjZz/7Weyyyy4xefLk+N3vfldU98orr8Q555wTJ5xwQuyyyy5Rt27dmDJlSrzyyivVflMOW4q+9wsrV66MO++8M3bbbbcYNmxYlTVHH310PPzww7Fo0aJo2bJl7LXXXjFt2rR45JFHok2bNtG4ceN1Hqm2cOHCwuVQymvSpEnsvvvuUVJSEpdddlkMGzYsjjvuuDj99NNjyZIlMXr06EqnVA0ZMiSuu+66GDx4cFx++eWx8847x2OPPVbol8rvZ//iF7+II488Mr7xjW/EKaecEtttt1188MEH8frrr8cLL7wQ999//1rbDZuLfqhYrVq14je/+U306dMnDjrooBg5cmQcdNBBsWzZsnjkkUfilltuiZ49e8Y111xTmGZ99nvywdiYMWMKZ9F17tw5Lr/88nj77bfjsMMOi+233z6WLFkS119/fdSpU6fSl8blXXbZZdG3b9/o06dPjBw5MsrKymLMmDFRWlq6WT63zpkzpxB8Ll68OB588MF48skn47jjjosdd9wxIlbvs914440xZMiQmDt3buy1117xxz/+Ma644oro169fHH744RERhWvq3X777dGhQ4fYe++94/nnn19nuLY2RxxxRPTo0SNGjRoVS5cuja5du8azzz5b6UCijz76KHr37h0nnXRSdOrUKRo3bhyzZs2Kxx9/vNojob8SvvTbXPybmDFjRhowYEDaZpttUu3atVOrVq1S//790/Tp0yvV5u+QsmjRomrHlff555+nH/zgB6lVq1apfv366cADD0wzZsxITZs2Lbp7TnV3iS0tLa3Rcm6//fbUsWPHVK9evbTTTjulK6+8Mt12220pItLf//73Qt3G3iX27LPPLhqWv7vM1VdfXTQ8vz73339/Ydhrr72W+vTpkxo3bpyaNWuWTjjhhDR//vxKd4xJKaWHHnoo7bXXXqlu3bqpXbt26aqrrkrf+973UrNmzSq19fbbb08HHHBAKi0tTQ0aNEgdOnRIJ598cpo9e/Y61xOy5KWXXkpDhgxJ7dq1S3Xr1k2lpaVp3333TRdffHF67733CnXrc4ewJ554In33u99NW221VWrQoEHq169f+r//+7+iafPv92nTpqWuXbumevXqpTZt2qT/+q//qnRHvRUrVqRx48alvffeO9WvXz81atQoderUKZ155plF8122bFkaOXJkpb6v4t1b1+a1114r3AVrzpw5ReNWrVpVuAPXrbfeWmnaqpZz9913p06dOqU6deoUba/16Wur286zZs0qqqvujmXVLaP8o3nz5umAAw5It99+e6U7F86bNy+ddNJJqUWLFqlOnTqpY8eO6eqrr65U9+6776YBAwak5s2bp6ZNm6bBgwen2bNnV3mX2Jqu+9tvv52OP/741KhRo9S4ceN0/PHHp+nTpxfNc+HChemUU05JnTp1SqWlpalRo0apc+fO6brrrksrV65c67aALUXfm9LEiRPXeYfnxx9/vOguhy+99FI6+OCDU8OGDVNErHP/smJfV/5x8MEHF9X+8pe/TLvsskuqW7du2nXXXdPtt9+ehgwZUnSX2JRSmj9/furfv39RvzR58uQq73r58ssvpxNPPDG1atUq1alTJ7Vu3Todeuih6eabb15ru+HLoB8q9v7776cLLrggderUqbCsbt26pRtuuCEtX768Un1N93uWLVuWhg0bllq2bJlyuVzhs/KkSZPSkUcembbbbrtUt27d1KpVq9SvX7/0zDPPFKat7q6qDz/8cOrcuXPR59aq9qNS2vDPrVXdJbZp06Zpn332Sddee22lu+ouXrw4nXXWWalNmzapdu3aqX379unCCy+sVPfRRx+lYcOGpW222SaVlpamo48+Os2dO7fau8RWzEDy7SqfNyxZsiSdeuqpaauttkoNGzZMffr0SW+88UbRPD///PN01llnpc6dO6cmTZqkBg0apI4dO6ZLLrkkLV26dK3bIstyKZU7NpyvrOnTp8fBBx8cd911V42ODGH14eD77LNPbLfddvHEE09s6eZA5k2YMCGGDh0as2bNiq5du661tlevXvH+++/HnDlzvqTWAXw96Xu3vCuuuCIuuuiimD9//ia/Jhh8FeiHYMtwSuxX0JNPPhkzZsyI/fbbLxo0aBAvv/xyXHXVVbHLLrt8tQ/33MxOO+206NOnT7Rp0yYWLFgQN998c7z++utx/fXXb+mmAQCQAfnLJXTq1ClWrFgRU6ZMif/+7/+OwYMHC+sA+FIJ7L6CmjRpEk888USMHz8+Pvnkk9h6663jyCOPjCuvvLLojq4U++STT+L888+PRYsWRZ06daJLly4xefLkwjn3AAD8e2vYsGFcd911MXfu3Fi2bFm0a9cufvSjH8VFF120pZsGwL8Zp8QCAAAAQIZsmluKAgAAAACbhMAOAAAAADJEYAcAAAAAGVLjm07kcrmi36t7RESUlJQU/Z3L5aKkpKRG0+ZrazJ+bfOraW1N5mW9rJf12rDaWrVqxddNxVvZ57d9eRXXO2s1VT0vFafb0jVZ34a2c3ZqtvQ2rElNt27dKg37Ohg9enREVO7/88P8n/w3Xa8ZM6Jk1KjIzZy5MS8vvq6+jpdPz+XWXQNQXg37QkfYAQAAAECGCOwAAAAAIEMEdgAAAACQIQI7AAAAAMgQgR0AAAAAZIjADgAAAAAyRGAHAAAAABkisAMAAACADBHYAQAAAECGCOwAAAAAIEMEdgAAAACQIQI7AAAAAMgQgR0AAAAAZIjADgAAAAAyRGAHAAAAABkisAMAAACADBHYAQAAAECGCOwAAAAAIEMEdgAAAACQIQI7AAAAAMiQ2lu6AQAAAMBqqdxjY+U2wTxYh9yGb2XPz5dkQ56jlCJS2qLPkcAOAAAAMiJFxLSI+EO5YblcLnJrQoein7nc6kBhzc+qaqqcrgbj1rumijbkf9/Q9m3wOmyO7ZKfZ1U1FZYTa8ZndZ3XVbPe61+D+W/S11o1bVtnfYVtVOV0uVzkUorctGkRTz8dW5LADgAAADIixeqw7mdrfs+HDiUlJUVBUElJSVHAUtOa3JphJRWHl5u+ZE2AsV7LKClZPV1++jW1uXLrsLE11a5PROHneteU/73CuHwb1jb9Omuq2oblg7H8+tWwpmhZVT0n63huN8lrY31fP2t73sv9zMprI1KKKCuL3DPPrP59CxHYAQAAQIakiFi15mduzSPK/YxcLlaVG7c+NfkL2ac1gUfkcpEioqRc/apYHZKsrSa/jEJNfrpyP9OamthENVW1J9+OSu2pYU2RCuMK22At06+rJlehPRW3a/nxNakpWla5YZVqKs63BjWF8WuCt0J7SkoK0xfms741FcetpY0bVFNFeypNX9OaVasqvza2ADedAAAAAIAMEdgBAAAAQIYI7AAAAAAgQwR2AAAAAJAhAjsAAAAAyBCBHQAAAABkiMAOAAAAADJEYAcAAAAAGSKwAwAAAIAMEdgBAAAAQIYI7AAAAAAgQwR2AAAAAJAhAjsAAAAAyBCBHQAAAABkiMAOAAAAADJEYAcAAAAAGSKwAwAAAIAMEdgBAAAAQIYI7AAAAAAgQwR2AAAAAJAhAjsAAAAAyBCBHQAAAABkiMAOAAAAADJEYAcAAAAAGSKwAwAAAIAMEdgBAAAAQIYI7AAAAAAgQwR2AAAAAJAhAjsAAAAAyBCBHQAAAABkiMAOAAAAADJEYAcAAAAAGSKwAwAAAIAMEdgBAAAAQIYI7AAAAAAgQwR2AAAAAJAhAjsAAAAAyBCBHQAAAABkiMAOAAAAADJEYAcAAAAAGSKwAwAAAIAMEdgBAAAAQIYI7AAAAAAgQwR2AAAAAJAhAjsAAAAAyBCBHQAAAABkiMAOAAAAADJEYAcAAAAAGSKwAwAAAIAMEdgBAAAAQIYI7AAAAAAgQwR2AAAAAJAhAjsAAAAAyBCBHQAAAABkiMAOAAAAADJEYAcAAAAAGSKwAwAAAIAMEdgBAAAAQIYI7AAAAAAgQwR2AAAAAJAhAjsAAAAAyBCBHQAAAABkiMAOAAAAADJEYAcAAAAAGSKwAwAAAIAMEdgBAAAAQIYI7AAAAAAgQwR2AAAAAJAhAjsAAAAAyBCBHQAAAABkiMAOAAAAADJEYAcAAAAAGSKwAwAAAIAMEdgBAAAAQIYI7AAAAAAgQwR2AAAAAJAhAjsAAAAAyBCBHQAAAABkiMAOAAAAADJEYAcAAAAAGSKwAwAAAIAMEdgBAAAAQIYI7AAAAAAgQwR2AAAAAJAhAjsAAAAAyBCBHQAAAABkiMAOAAAAADJEYAcAAAAAGSKwAwAAAIAMEdgBAAAAQIYI7AAAAAAgQwR2AAAAAJAhAjsAAAAAyBCBHQAAAABkiMAOAAAAADJEYAcAAAAAGSKwAwAAAIAMEdgBAAAAQIYI7AAAAAAgQwR2AAAAAJAhAjsAAAAAyBCBHQAAAABkiMAOAAAAADJEYAcAAAAAGSKwAwAAAIAMEdgBAAAAQIYI7AAAAAAgQwR2AAAAAJAhAjsAAAAAyJDaW7oBAAB89bSdPz8iInK53OrH6j8Kv+dyuaK/C7/ncl9Mt2Z4Sb624nRraks2cF6FeZaUFE1Xsd258ssvKSkev57zKtoG5edVod3rGl/teq2lNv9zXdug0vgq5rs+26iods6ciI8/rtmLCAColsAOAID1dsSTTxZ+zxV+yRUPK/f3xgzb5POq6bBNOa91zD8L89roNkREfPppxJowFwDYcAI7AADWW9u3397STQAA+NpyDTsAAAAAyBCBHQAAAABkiMAOAAAAADJEYAcAAAAAGSKwAwAAAIAMEdgBAAAAQIYI7AAAAAAgQwR2AAAAAJAhAjsAAAAAyBCBHQAAAABkiMAOAAAAADJEYAcAAAAAGSKwAwAAAIAMEdgBAAAAQIYI7AAAAAAgQwR2AAAAAJAhAjsAAAAAyBCBHQAAAABkiMAOAAAAADJEYAcAAAAAGSKwAwAAAIAMEdgBAAAAQIYI7AAAAAAgQwR2AAAAAJAhAjsAAAAAyBCBHQAAAABkiMAOAAAAADJEYAcAAAAAGSKwAwAAAIAMEdgBAAAAQIYI7AAAAAAgQwR2AAAAAJAhAjsAAAAAyBCBHQAAAABkiMAOAAAAADJEYAcAAAAAGSKwAwAAAIAMEdgBAAAAQIYI7AAAAAAgQwR2AAAAAJAhAjsAAAAAyBCBHQAAAABkiMAOAAAAADJEYAcAAAAAGSKwAwAAAIAMEdgBAAAAQIYI7AAAAAAgQwR2AAAAAJAhAjsAAAAAyBCBHQAAAABkiMAOAAAAADJEYAcAAAAAGSKwAwAAAIAMEdgBAAAAQIYI7AAAAAAgQwR2AAAAAJAhAjsAAAAAyBCBHQAAAABkiMAOAAAAADJEYAcAAAAAGSKwAwAAAIAMEdgBAAAAQIYI7AAAAAAgQwR2AAAAAJAhAjsAAAAAyBCBHQAAAABkiMAOAAAAADJEYAcAAAAAGSKwAwAAAIAMEdgBAAAAQIYI7AAAAAAgQwR2AAAAAJAhAjsAAAAAyBCBHQAAAABkiMAOAAAAADJEYAcAAAAAGSKwAwAAAIAMEdgBAAAAQIYI7AAAAAAgQwR2AAAAAJAhAjsAAAAAyBCBHQAAAABkiMAOAAAAADJEYAcAAAAAGSKwAwAAAIAMEdgBAAAAQIYI7AAAAAAgQwR2AAAAAJAhAjsAAAAAyBCBHQAAAABkiMAOAAAAADJEYAcAAAAAGSKwAwAAAIAMEdgBAAAAQIYI7AAAAAAgQwR2AAAAAJAhAjsAAAAAyBCBHQAAAABkiMAOAAAAADJEYAcAAAAAGSKwAwAAAIAMEdgBAAAAQIYI7AAAAAAgQwR2AAAAAJAhAjsAAAAAyBCBHQAAAABkiMAOAAAAADJEYAcAAAAAGSKwAwAAAIAMqb2lGwAAAAB8oWTNI8UXR9mUREQu/0gpcrlcYVx+eFU1ufLDytek9MXwXC5yKUVJ+fqU1lkT5WpKqllWpPTFOlRVk1LldaiuHdW0p/wy1lZTNM/88vLT52vK11WsqWoZ5de14vaPiNyqVavbkX+sWhW5kpLi6VetisjlItbURMWaNdsocrnK0+bHpxS5kpLCz2pr8supqibfjjXLqtie8stYa82a4UU1JSWrt0kV069vTZXLqKI9FactyNekFFFSUlyzZjsVlr8FCewAAAAgI3IR0SvKnQ5XLviJWB1CxZq/c6sHFIbnyv1e3c8NHfdlTF+pZs361XRcoSY/bi01m63NX4VlrG3bbez0a8aVn6ZGdRXnuTmWUbGmimXkY73c009v8dBOYAcAAAAZkYuInhHRo/zAjBzx83WRW3fJV0/ua7lWW04G3m8COwAAAMiIXIWfUCMZCJjYtNx0AgAAAAAyRGAHAAAAABkisAMAAACADBHYAQAAAECGCOwAAAAAIEMEdgAAAACQIQI7AAAAAMgQgR0AAAAAZIjADgAAAAAyRGAHAAAAABkisAMAAACADBHYAQAAAECGCOwAAAAAIEMEdgAAAACQIQI7AAAAAMgQgR0AAAAAZIjADgAAAAAyRGAHAAAAABkisAMAAACADBHYAQAAAECG5FJKaUs3AgAAAABYzRF2AAAAAJAhAjsAAAAAyBCBHQAAAABkiMAOAAAAADJEYAcAAAAAGSKwAwAAAIAMEdgBAAAAQIYI7AAAAAAgQwR2AAAAAJAh/w/yu8ao+3tyCgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1600x400 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a demo image (100x100) with a gradient and a solid square in the center\n",
    "image = np.zeros((100, 100, 3), dtype=np.uint8)\n",
    "cv2.rectangle(image, (30, 30), (70, 70), (255, 255, 255), -1)  # White square\n",
    "for i in range(100):\n",
    "    image[:, i] = i * 2.55  # Gradient from black to white\n",
    "\n",
    "# Define background color\n",
    "background_color = (0, 0, 255)  # Red background\n",
    "\n",
    "# Perform clipping operations\n",
    "clipped_image_within, _, _ = safe_clip(image, 20, 20, 60, 60, background_color)\n",
    "clipped_image_edge, _, _ = safe_clip(image, 50, 50, 100, 100, background_color)\n",
    "clipped_image_outside, _, _ = safe_clip(image, -10, -10, 120, 120, background_color)\n",
    "\n",
    "# Setup matplotlib plots\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
    "axs[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "axs[0].set_title('Original Image')\n",
    "axs[0].axis('off')\n",
    "\n",
    "axs[1].imshow(cv2.cvtColor(clipped_image_within, cv2.COLOR_BGR2RGB))\n",
    "axs[1].set_title('Clipped Within Bounds')\n",
    "axs[1].axis('off')\n",
    "\n",
    "axs[2].imshow(cv2.cvtColor(clipped_image_edge, cv2.COLOR_BGR2RGB))\n",
    "axs[2].set_title('Clipped At Edge')\n",
    "axs[2].axis('off')\n",
    "\n",
    "axs[3].imshow(cv2.cvtColor(clipped_image_outside, cv2.COLOR_BGR2RGB))\n",
    "axs[3].set_title('Clipped Outside Bounds')\n",
    "axs[3].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5g--vkll2de"
   },
   "source": [
    "## Provide Unit Tests\n",
    "\n",
    "Unit testing is an essential facet of software engineering. Unit testing is necessary when a prompt generates your code. The following code shows how I converted the previous three examples into unit tests.\n",
    "\n",
    "Most software architects suggest you should not use an LLM to generate the unit tests because you must know what is being tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PFT-XSZ-l5U7"
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "class TestSafeClip(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        self.image = np.zeros((100, 100, 3), dtype=np.uint8)\n",
    "        cv2.rectangle(self.image, (30, 30), (70, 70), (255, 255, 255), -1)\n",
    "        for i in range(100):\n",
    "            self.image[:, i] = i * 2.55  # Gradient from black to white\n",
    "        self.background_color = (0, 0, 255)  # Blue background\n",
    "\n",
    "    def test_clip_within_bounds(self):\n",
    "        clipped_image, x_off, y_off = safe_clip(self.image, 20, 20, 60, 60, self.background_color)\n",
    "        self.assertEqual(clipped_image.shape, (60, 60, 3))\n",
    "        self.assertEqual((x_off, y_off), (0, 0))\n",
    "        self.assertTrue((clipped_image[0, 0] != self.background_color).all())\n",
    "\n",
    "    def test_clip_at_edge(self):\n",
    "        clipped_image, x_off, y_off = safe_clip(self.image, 50, 50, 100, 100, self.background_color)\n",
    "        self.assertEqual(clipped_image.shape, (100, 100, 3))\n",
    "        self.assertEqual((x_off, y_off), (0, 0))\n",
    "        self.assertTrue((clipped_image[0, 0] == [127, 127, 127]).all())\n",
    "\n",
    "    def test_clip_outside_bounds(self):\n",
    "        clipped_image, x_off, y_off = safe_clip(self.image, -10, -10, 120, 120, self.background_color)\n",
    "        self.assertEqual(clipped_image.shape, (120, 120, 3))\n",
    "        self.assertEqual((x_off, y_off), (10, 10))\n",
    "        self.assertTrue((clipped_image[10, 10] == [0, 0, 0]).all())\n",
    "\n",
    "test = TestSafeClip()\n",
    "test.setUp()\n",
    "test.test_clip_within_bounds()\n",
    "test.test_clip_at_edge()\n",
    "test.test_clip_outside_bounds()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "generative-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
